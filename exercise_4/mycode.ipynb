{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于神经网络的多分类问题  \n",
    "这里的分类还20*20像素的手写数字  \n",
    "前置神经网络感觉就像是多重逻辑回归的组合体  \n",
    "输入是400个特征加一个常量，输入到隐藏层的25个神经元（也就是25个逻辑回归）  \n",
    "到隐藏层后是25个特征加一个常量，输入到10个输出层（也就是10个逻辑回归）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat#读取mat文件\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "def getData(url):\n",
    "    data=loadmat(url)\n",
    "    x=numpy.array(data['X'])\n",
    "    x=numpy.insert(x,0,numpy.ones(x.shape[0]),axis=1)\n",
    "    \n",
    "    #y还是要加工一下，变成5000*10的形式，方便再求代价的时候直接与向前传播的结果进行计算\n",
    "    ynum=numpy.array(data['y'])\n",
    "    y=numpy.zeros((x.shape[0],10))\n",
    "    for i in range(len(ynum)):\n",
    "        y[i][ynum[i]-1]=1\n",
    "\n",
    "    #theta1和theta2分别是输入到隐藏层的和隐藏到输出层的,神经网络初始权重不能全为0\n",
    "    #theta1=numpy.array(numpy.zeros((25,x.shape[1])))\n",
    "    #theta2=numpy.array(numpy.zeros((10,26)))#这里直接定义好，隐藏层神经元25个\n",
    "    # theta1=numpy.random.random((25,x.shape[1]))\n",
    "    # theta2=numpy.random.random((10,26))\n",
    "\n",
    "    #试试继续下降\n",
    "    theta1=numpy.array(numpy.load('theta1.npy'))\n",
    "    theta2=numpy.array(numpy.load('theta2.npy'))\n",
    "    return x,y,theta1,theta2\n",
    "#getData('ex4data1.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先把基本的函数定义出来，代价函数和梯度下降函数  \n",
    "# 代价函数  \n",
    "K是分类的数量（就是10）  \n",
    "<img style=\"float: left;\" src=\"nn_regcost.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10285,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 235 into shape (10,26)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\GithubWorkspace\\MachineLearningHomework\\exercise_4\\mycode.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 94>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GithubWorkspace/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m params\u001b[39m=\u001b[39mnumpy\u001b[39m.\u001b[39mconcatenate((numpy\u001b[39m.\u001b[39mravel(theta1), numpy\u001b[39m.\u001b[39mravel(theta2)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GithubWorkspace/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39mprint\u001b[39m(params\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GithubWorkspace/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m fmin \u001b[39m=\u001b[39m minimize(fun\u001b[39m=\u001b[39;49mbackPropagate, x0\u001b[39m=\u001b[39;49mparams, args\u001b[39m=\u001b[39;49m(x, y), method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTNC\u001b[39;49m\u001b[39m'\u001b[39;49m, jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, options\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mmaxiter\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m250\u001b[39;49m})\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GithubWorkspace/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m fmin\n",
      "File \u001b[1;32mD:\\Python3\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:684\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    681\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    682\u001b[0m                            callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    683\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 684\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    685\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    686\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcobyla\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    687\u001b[0m     res \u001b[39m=\u001b[39m _minimize_cobyla(fun, x0, args, constraints, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    688\u001b[0m                             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mD:\\Python3\\Lib\\site-packages\\scipy\\optimize\\_tnc.py:377\u001b[0m, in \u001b[0;36m_minimize_tnc\u001b[1;34m(fun, x0, args, jac, bounds, eps, scale, offset, mesg_num, maxCGit, maxiter, eta, stepmx, accuracy, minfev, ftol, xtol, gtol, rescale, disp, callback, finite_diff_rel_step, maxfun, **unknown_options)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    375\u001b[0m     messages \u001b[39m=\u001b[39m MSG_NONE\n\u001b[1;32m--> 377\u001b[0m sf \u001b[39m=\u001b[39m _prepare_scalar_function(fun, x0, jac\u001b[39m=\u001b[39;49mjac, args\u001b[39m=\u001b[39;49margs, epsilon\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    378\u001b[0m                               finite_diff_rel_step\u001b[39m=\u001b[39;49mfinite_diff_rel_step,\n\u001b[0;32m    379\u001b[0m                               bounds\u001b[39m=\u001b[39;49mnew_bounds)\n\u001b[0;32m    380\u001b[0m func_and_grad \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39mfun_and_grad\n\u001b[0;32m    382\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39mlow, up   : the bounds (lists of floats)\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[39m            if low is None, the lower bounds are removed.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[39m            if up is None, the upper bounds are removed.\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[39m            low and up defaults to None\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Python3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:263\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    259\u001b[0m     bounds \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39minf)\n\u001b[0;32m    261\u001b[0m \u001b[39m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[39m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m sf \u001b[39m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0;32m    264\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[39m=\u001b[39;49mepsilon)\n\u001b[0;32m    266\u001b[0m \u001b[39mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32mD:\\Python3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx)\n\u001b[0;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun_impl \u001b[39m=\u001b[39m update_fun\n\u001b[1;32m--> 158\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[0;32m    160\u001b[0m \u001b[39m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mif\u001b[39;00m callable(grad):\n",
      "File \u001b[1;32mD:\\Python3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Python3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[1;32mD:\\Python3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mD:\\Python3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[0;32m     75\u001b[0m     \u001b[39m\"\"\" returns the the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_if_needed(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[1;32mD:\\Python3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> 70\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[0;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32md:\\GithubWorkspace\\MachineLearningHomework\\exercise_4\\mycode.ipynb Cell 4\u001b[0m in \u001b[0;36mbackPropagate\u001b[1;34m(params, x, y)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GithubWorkspace/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackPropagate\u001b[39m(params,x,y):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GithubWorkspace/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m#先来一个下降，之后再整合\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GithubWorkspace/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m#5000*10的矩阵(第一个样本，第一个输出的求导)、5000*25的矩阵(第一个样本，对第一个权重求导)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GithubWorkspace/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     theta1 \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(numpy\u001b[39m.\u001b[39mreshape(params[:\u001b[39m25\u001b[39m \u001b[39m*\u001b[39m (\u001b[39m401\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)], (\u001b[39m25\u001b[39m, (\u001b[39m401\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GithubWorkspace/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     theta2 \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(numpy\u001b[39m.\u001b[39;49mreshape(params[\u001b[39m25\u001b[39;49m \u001b[39m*\u001b[39;49m (\u001b[39m401\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m):], (\u001b[39m10\u001b[39;49m, (\u001b[39m25\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m))))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GithubWorkspace/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     hide,output\u001b[39m=\u001b[39mforwardPropagate(x,theta1,theta2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GithubWorkspace/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     totalError\u001b[39m=\u001b[39moutput\u001b[39m-\u001b[39my\u001b[39m#5000*10\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mD:\\Python3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[0;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape\u001b[39m(a, newshape, order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    200\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39m           [5, 6]])\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39mreshape\u001b[39;49m\u001b[39m'\u001b[39;49m, newshape, order\u001b[39m=\u001b[39;49morder)\n",
      "File \u001b[1;32mD:\\Python3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 235 into shape (10,26)"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+numpy.exp(-z))\n",
    "\n",
    "#向前传播\n",
    "def forwardPropagate(x,theta1,theta2):\n",
    "    hide=sigmoid(x@theta1.T)#隐藏层的值\n",
    "    hide=numpy.insert(hide,0,numpy.ones(hide.shape[0]),axis=1)#插入一列1\n",
    "    output=sigmoid(hide@theta2.T)#最终1000样本输出的结果，1000*10\n",
    "    return hide,output\n",
    "\n",
    "'''\n",
    "这里的代价函数看公式是把所有的逻回归的代价求和\n",
    "单个逻辑回归，知道计算出来的结果和实际结果\n",
    "但是现在神经网络加了一个隐藏层\n",
    "（可以知道的是输入和输出以及正确结果，但是隐藏层的正确结果不知道）\n",
    "看答案的代价函数，只考虑输入与输出，没有计算隐藏层怎么样\n",
    "\n",
    "'''\n",
    "\n",
    "def cost(output,y,theta1,theta2,L):\n",
    "    price=(-y*numpy.log(output)-(1-y)*numpy.log(1-output)).sum()/output.shape[0]#代价\n",
    "    regul=(numpy.power(theta1,2).sum()+numpy.power(theta2,2).sum())*(L/(2*output.shape[0]))#正则化\n",
    "    return price+regul\n",
    "\n",
    "'''\n",
    "然后开始反向传播\n",
    "反向传播前先写好基础的sigmoid函数的梯度下降公式，简单求导可知\n",
    "'''\n",
    "def gradientSigmoid(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "'''\n",
    "理解反向传播，见笔记新增的连接与图片\n",
    "代价函数求得的代价是最终结果的误差\n",
    "这个误差要不断的向后去寻找误差源，简单来说，传播过程中权重比较大的值就说明他贡献的误差较大\n",
    "以此来不断的向后去更新他们的权重\n",
    "最终的目的就是求总体对改权重的偏导(通过链式求导法则)\n",
    "'''\n",
    "def backPropagate(params,x,y):\n",
    "    #先来一个下降，之后再整合\n",
    "    #5000*10的矩阵(第一个样本，第一个输出的求导)、5000*25的矩阵(第一个样本，对第一个权重求导)\n",
    "    theta1 = numpy.array(numpy.reshape(params[:25 * (401 + 1)], (25, (401 + 1))))\n",
    "    theta2 = numpy.array(numpy.reshape(params[25 * (401 + 1):], (10, (25 + 1))))\n",
    "\n",
    "    hide,output=forwardPropagate(x,theta1,theta2)\n",
    "\n",
    "    totalError=output-y#5000*10\n",
    "    z1=x@theta1.T#5000*25\n",
    "    dhz1=gradientSigmoid(z1)#5000*25\n",
    "    L=1\n",
    "\n",
    "    '''\n",
    "    对比答案上的梯度下降\n",
    "    答案的关于theta1的:\n",
    "    ((theta2 @ totalError) * dhz1) @ x\n",
    "\n",
    "    我自己写的关于theta2的:\n",
    "    (totalError @ theta2) * (dhz1 @ x)\n",
    "\n",
    "    看起来是一样的啊\n",
    "    '''\n",
    "\n",
    "    decline1=numpy.zeros((theta1.shape[0],theta1.shape[1]))#25*401\n",
    "    decline2=numpy.zeros((theta2.shape[0],theta2.shape[1]))#10*26\n",
    "    for i in range(totalError.shape[0]):#对5000个样本进行下降\n",
    "        decline2+=(totalError[i].reshape(-1,1) * theta2)\n",
    "                                #1*10      @      10*26  *  25       @          401\n",
    "        decline1+=numpy.multiply((totalError[i].reshape(-1,1).T@theta2[:,1:]).T,(dhz1[i].reshape(-1,1)@x[i].reshape(-1,1).T))\n",
    "\n",
    "        decline1[:,1:]+=(theta1[:,1:]*L/5000)\n",
    "        decline2[:,1:]+=(theta2[:,1:]*L/5000)\n",
    "    # theta1-=0.0001*decline1\n",
    "    # theta2-=0.0001*decline2\n",
    "    hide,output=forwardPropagate(x,theta1,theta2)\n",
    "    params=numpy.concatenate((numpy.ravel(decline1), numpy.ravel(decline2)))\n",
    "    J=cost(output,y,theta1,theta2,L)\n",
    "\n",
    "    return J,\n",
    "\n",
    "def test():\n",
    "    x,y,theta1,theta2=getData('ex4data1.mat')\n",
    "    params=numpy.concatenate((numpy.ravel(theta1), numpy.ravel(theta2)))\n",
    "    print(params.shape)\n",
    "    costs,theta1,theta2=backPropagate(params,x,y)\n",
    "    # numpy.save('theta1',theta1)\n",
    "    # numpy.save('theta2',theta2)\n",
    "\n",
    "#test()\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "x,y,theta1,theta2=getData('ex4data1.mat')\n",
    "params=numpy.concatenate((numpy.ravel(theta1), numpy.ravel(theta2)))\n",
    "print(params.shape)\n",
    "fmin = minimize(fun=backPropagate, x0=params, args=(x, y), method='TNC', jac=True, options={'maxiter': 250})\n",
    "fmin\n",
    "\n",
    "\n",
    "#测试测试这个梯度下降的效果好不好，虽然学过头了\n",
    "# x,y,theta1,theta2=getData('ex4data1.mat')\n",
    "# theta1=numpy.array(numpy.load('theta1.npy'))\n",
    "# theta2=numpy.array(numpy.load('theta2.npy'))\n",
    "# print(x.shape,theta1.shape,theta2.shape)\n",
    "# hide,output=forwardPropagate(x,theta1,theta2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08f6edec437538b5993be24a8d19dd1bd66f0a7ae79d727201cebd80cd580c21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
