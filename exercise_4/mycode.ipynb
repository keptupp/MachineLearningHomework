{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于神经网络的多分类问题  \n",
    "这里的分类还20*20像素的手写数字  \n",
    "前置神经网络感觉就像是多重逻辑回归的组合体  \n",
    "输入是400个特征加一个常量，输入到隐藏层的25个神经元（也就是25个逻辑回归）  \n",
    "到隐藏层后是25个特征加一个常量，输入到10个输出层（也就是10个逻辑回归）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat#读取mat文件\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "def getData(url):\n",
    "    data=loadmat(url)\n",
    "    x=numpy.array(data['X'])\n",
    "    x=numpy.insert(x,0,numpy.ones(x.shape[0]),axis=1)\n",
    "    \n",
    "    #y还是要加工一下，变成5000*10的形式，方便再求代价的时候直接与向前传播的结果进行计算\n",
    "    ynum=numpy.array(data['y'])\n",
    "    y=numpy.zeros((x.shape[0],10))\n",
    "    for i in range(len(ynum)):\n",
    "        y[i][ynum[i]-1]=1\n",
    "\n",
    "    #theta1和theta2分别是输入到隐藏层的和隐藏到输出层的,神经网络初始权重不能全为0\n",
    "    #theta1=numpy.array(numpy.zeros((25,x.shape[1])))\n",
    "    #theta2=numpy.array(numpy.zeros((10,26)))#这里直接定义好，隐藏层神经元25个\n",
    "    theta1=(numpy.random.random((25,x.shape[1]))-0.5)*0.25\n",
    "    theta2=(numpy.random.random((10,26))-0.5)*0.25\n",
    "\n",
    "    #试试继续下降\n",
    "    # theta1=numpy.array(numpy.load('theta1.npy'))\n",
    "    # theta2=numpy.array(numpy.load('theta2.npy'))\n",
    "    return x,y,theta1,theta2\n",
    "#getData('ex4data1.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先把基本的函数定义出来，代价函数和梯度下降函数  \n",
    "# 代价函数  \n",
    "K是分类的数量（就是10）  \n",
    "<img style=\"float: left;\" src=\"nn_regcost.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 7.317497897671459\n",
      "     jac: array([ 8.68870187e-03, -1.06387239e-05, -1.05363802e-05, ...,\n",
      "        1.34447312e+04,  1.34447312e+04,  1.34447312e+04])\n",
      " message: 'Linear search failed'\n",
      "    nfev: 43\n",
      "     nit: 0\n",
      "  status: 4\n",
      " success: False\n",
      "       x: array([ 0.12418582, -0.05319362, -0.0526819 , ..., -0.11221358,\n",
      "        0.04209831,  0.05501453])\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+numpy.exp(-z))\n",
    "\n",
    "#向前传播\n",
    "def forwardPropagate(x,theta1,theta2):\n",
    "    hide=sigmoid(x@theta1.T)#隐藏层的值\n",
    "    hide=numpy.insert(hide,0,numpy.ones(hide.shape[0]),axis=1)#插入一列1\n",
    "    output=sigmoid(hide@theta2.T)#最终1000样本输出的结果，1000*10\n",
    "    return hide,output\n",
    "\n",
    "'''\n",
    "这里的代价函数看公式是把所有的逻回归的代价求和\n",
    "单个逻辑回归，知道计算出来的结果和实际结果\n",
    "但是现在神经网络加了一个隐藏层\n",
    "（可以知道的是输入和输出以及正确结果，但是隐藏层的正确结果不知道）\n",
    "看答案的代价函数，只考虑输入与输出，没有计算隐藏层怎么样\n",
    "\n",
    "'''\n",
    "\n",
    "def cost(output,y,theta1,theta2,L):\n",
    "    price=(-y*numpy.log(output)-(1-y)*numpy.log(1-output)).sum()/output.shape[0]#代价\n",
    "    regul=(numpy.power(theta1,2).sum()+numpy.power(theta2,2).sum())*(L/(2*output.shape[0]))#正则化\n",
    "    return price+regul\n",
    "\n",
    "'''\n",
    "然后开始反向传播\n",
    "反向传播前先写好基础的sigmoid函数的梯度下降公式，简单求导可知\n",
    "'''\n",
    "def gradientSigmoid(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "'''\n",
    "理解反向传播，见笔记新增的连接与图片\n",
    "代价函数求得的代价是最终结果的误差\n",
    "这个误差要不断的向后去寻找误差源，简单来说，传播过程中权重比较大的值就说明他贡献的误差较大\n",
    "以此来不断的向后去更新他们的权重\n",
    "最终的目的就是求总体对改权重的偏导(通过链式求导法则)\n",
    "'''\n",
    "def backPropagate(params,x,y):\n",
    "    #先来一个下降，之后再整合\n",
    "    #5000*10的矩阵(第一个样本，第一个输出的求导)、5000*25的矩阵(第一个样本，对第一个权重求导)\n",
    "    theta1 = numpy.array(numpy.reshape(params[:25 * (400 + 1)], (25, (400 + 1))))\n",
    "\n",
    "    theta2 = numpy.array(numpy.reshape(params[25 * (400 + 1):], (10, (25 + 1))))\n",
    "\n",
    "    hide,output=forwardPropagate(x,theta1,theta2)\n",
    "\n",
    "    totalError=output-y#5000*10\n",
    "    z1=x@theta1.T#5000*25\n",
    "    dhz1=gradientSigmoid(z1)#5000*25\n",
    "    L=1\n",
    "\n",
    "    '''\n",
    "    对比答案上的梯度下降\n",
    "    答案的关于theta1的:\n",
    "    ((theta2 @ totalError) * dhz1) @ x\n",
    "\n",
    "    我自己写的关于theta2的:\n",
    "    (totalError @ theta2) * (dhz1 @ x)\n",
    "\n",
    "    看起来是一样的啊\n",
    "    '''\n",
    "\n",
    "    decline1=numpy.zeros((theta1.shape[0],theta1.shape[1]))#25*401\n",
    "    decline2=numpy.zeros((theta2.shape[0],theta2.shape[1]))#10*26\n",
    "    for i in range(totalError.shape[0]):#对5000个样本进行下降\n",
    "        decline2=decline2+(totalError[i].reshape(-1,1).T @ totalError[i].reshape(-1,1))\n",
    "                                #1*10      @      10*26  *  25       @          401\n",
    "        #decline1=decline1+numpy.multiply((totalError[i].reshape(-1,1).T@theta2[:,1:]).T,(dhz1[i].reshape(-1,1)@x[i].reshape(-1,1).T))\n",
    "        decline1=((totalError[i].reshape(-1,1).T@theta2[:,1:]).T*dhz1[i].reshape(-1,1))@x[i].reshape(-1,1).T\n",
    "    decline1[:,1:]=decline1[:,1:]+(theta1[:,1:]*L/5000)\n",
    "    decline2[:,1:]=decline2[:,1:]+(theta2[:,1:]*L/5000)\n",
    "    # theta1-=0.0001*decline1\n",
    "    # theta2-=0.0001*decline2\n",
    "    grad=numpy.concatenate((numpy.ravel(decline1), numpy.ravel(decline2)))\n",
    "    J=cost(output,y,theta1,theta2,L)\n",
    "\n",
    "    return J,grad\n",
    "\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "import scipy\n",
    "def main():\n",
    "\n",
    "    x,y,theta1,theta2=getData('ex4data1.mat')\n",
    "    params=numpy.array(numpy.concatenate((numpy.ravel(theta1), numpy.ravel(theta2))))\n",
    "   \n",
    "    fmin = minimize(fun=backPropagate, x0=params, args=(x, y), method='TNC', jac=True, options={'maxiter': 1000})\n",
    "    print(fmin)\n",
    "    params=numpy.array(fmin.x)\n",
    "    theta1 = numpy.array(numpy.reshape(params[:25 * (400 + 1)], (25, (400 + 1))))\n",
    "    theta2 = numpy.array(numpy.reshape(params[25 * (400 + 1):], (10, (25 + 1))))\n",
    "    numpy.save('theta1',theta1)\n",
    "    numpy.save('theta2',theta2)\n",
    "    hide,output=forwardPropagate(x,theta1,theta2)\n",
    "    return hide,output\n",
    "hide,output=main()\n",
    "\n",
    "def test():\n",
    "    x,y,theta1,theta2=getData('ex4data1.mat')\n",
    "    params=numpy.array(numpy.concatenate((numpy.ravel(theta1), numpy.ravel(theta2))))\n",
    "    J,grad=backPropagate(params,x,y)\n",
    "    print(J)\n",
    "\n",
    "#test()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08f6edec437538b5993be24a8d19dd1bd66f0a7ae79d727201cebd80cd580c21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
