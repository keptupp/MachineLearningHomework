{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于神经网络的多分类问题  \n",
    "这里的分类还20*20像素的手写数字  \n",
    "前置神经网络感觉就像是多重逻辑回归的组合体  \n",
    "输入是400个特征加一个常量，输入到隐藏层的25个神经元（也就是25个逻辑回归）  \n",
    "到隐藏层后是25个特征加一个常量，输入到10个输出层（也就是10个逻辑回归）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat#读取mat文件\n",
    "import numpy\n",
    "def getData(url):\n",
    "    data=loadmat(url)\n",
    "    x=numpy.array(data['X'])\n",
    "    x=numpy.insert(x,0,numpy.ones(x.shape[0]),axis=1)\n",
    "    \n",
    "    #y还是要加工一下，变成5000*10的形式，方便再求代价的时候直接与向前传播的结果进行计算\n",
    "    ynum=numpy.array(data['y'])\n",
    "    y=numpy.zeros((x.shape[0],10))\n",
    "    for i in range(len(ynum)):\n",
    "        y[i][ynum[i]-1]=1\n",
    "\n",
    "    #theta1和theta2分别是输入到隐藏层的和隐藏到输出层的,神经网络初始权重不能全为0\n",
    "    #theta1=numpy.array(numpy.zeros((25,x.shape[1])))\n",
    "    #theta2=numpy.array(numpy.zeros((10,26)))#这里直接定义好，隐藏层神经元25个\n",
    "    theta1=numpy.random.random((25,x.shape[1]))\n",
    "    theta2=numpy.random.random((10,26))\n",
    "    return x,y,theta1,theta2\n",
    "#getData('ex4data1.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先把基本的函数定义出来，代价函数和梯度下降函数  \n",
    "# 代价函数  \n",
    "K是分类的数量（就是10）  \n",
    "<img style=\"float: left;\" src=\"nn_regcost.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\PythonCode\\MachineLearningHomework\\exercise_4\\mycode.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PythonCode/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mprint\u001b[39m(cost(output,y,theta1,theta2,\u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PythonCode/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39mprint\u001b[39m(backPropagate(x,hide,output,y,theta1,theta2))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/PythonCode/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m test()\n",
      "\u001b[1;32mc:\\PythonCode\\MachineLearningHomework\\exercise_4\\mycode.ipynb Cell 4\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PythonCode/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest\u001b[39m():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/PythonCode/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     x,y,theta1,theta2\u001b[39m=\u001b[39mgetData(\u001b[39m'\u001b[39m\u001b[39mex4data1.mat\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PythonCode/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     hide,output\u001b[39m=\u001b[39mforwardPropagate(x,theta1,theta2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PythonCode/MachineLearningHomework/exercise_4/mycode.ipynb#W3sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mprint\u001b[39m(cost(output,y,theta1,theta2,\u001b[39m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'getData' is not defined"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+numpy.exp(-z))\n",
    "\n",
    "#向前传播\n",
    "def forwardPropagate(x,theta1,theta2):\n",
    "    hide=sigmoid(x@theta1.T)#隐藏层的值\n",
    "    hide=numpy.insert(hide,0,numpy.ones(hide.shape[0]),axis=1)#插入一列1\n",
    "    output=sigmoid(hide@theta2.T)#最终1000样本输出的结果，1000*10\n",
    "    return hide,output\n",
    "\n",
    "'''\n",
    "这里的代价函数看公式是把所有的逻回归的代价求和\n",
    "单个逻辑回归，知道计算出来的结果和实际结果\n",
    "但是现在神经网络加了一个隐藏层\n",
    "（可以知道的是输入和输出以及正确结果，但是隐藏层的正确结果不知道）\n",
    "看答案的代价函数，只考虑输入与输出，没有计算隐藏层怎么样\n",
    "\n",
    "'''\n",
    "\n",
    "def cost(output,y,theta1,theta2,L):\n",
    "    price=(-y*numpy.log(output)-(1-y)*numpy.log(1-output)).sum()/output.shape[0]#代价\n",
    "    regul=(numpy.power(theta1,2).sum()+numpy.power(theta2,2).sum())*(L/(2*output.shape[0]))#正则化\n",
    "    return price+regul\n",
    "\n",
    "'''\n",
    "然后开始反向传播\n",
    "反向传播前先写好基础的sigmoid函数的梯度下降公式，简单求导可知\n",
    "'''\n",
    "def gradientSigmoid(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "'''\n",
    "理解反向传播，见笔记新增的连接与图片\n",
    "代价函数求得的代价是最终结果的误差\n",
    "这个误差要不断的向后去寻找误差源，简单来说，传播过程中权重比较大的值就说明他贡献的误差较大\n",
    "以此来不断的向后去更新他们的权重\n",
    "最终的目的就是求总体对改权重的偏导(通过链式求导法则)\n",
    "'''\n",
    "def backPropagate(x,hide,output,y,theta1,theta2):\n",
    "    errorValue2=(output-y)*gradientSigmoid(hide@theta2.T)#一个5000*10的矩阵 \n",
    "    #先来一个下降，之后再整合\n",
    "    #5000*10的矩阵(第一个样本，第一个输出的求导)、5000*25的矩阵(第一个样本，对第一个权重求导)\n",
    "    #theta1[0][0]表示对输入隐藏层第一个神经元第一个权重的求导\n",
    "    gradientSigmoid(hide@theta2.T)[0][0]*gradientSigmoid(x@theta1.T)[0][0]*theta1[0][0]\n",
    "    gradientSigmoid(hide@theta2.T)[0][1-10]*gradientSigmoid(x@theta1.T)[0][0]*theta1[0][0]*theta2[10][26]\n",
    "\n",
    "    #一行10个\n",
    "    gradientSigmoid(hide@theta2.T)[0]*gradientSigmoid(x@theta1.T)[0][0]\n",
    "\n",
    "\n",
    "    decline2=numpy.zeros((theta2.shape[0],theta2.shape[1]))\n",
    "    for i in range(errorValue2.shape[0]):#对5000个样本进行下降\n",
    "        decline2+=(errorValue2[i].reshape(-1,1) * theta2[:,:])\n",
    "    return decline2\n",
    "\n",
    "def test():\n",
    "    x,y,theta1,theta2=getData('ex4data1.mat')\n",
    "    hide,output=forwardPropagate(x,theta1,theta2)\n",
    "    print(cost(output,y,theta1,theta2,1))\n",
    "    print(backPropagate(x,hide,output,y,theta1,theta2))\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1daca1ebfa99d363f49187de6dd878fa6001e85bc2f160c60bcd81d0df0caa1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
