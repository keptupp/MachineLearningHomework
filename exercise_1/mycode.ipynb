{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一个变量的线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入库\n",
    "numpy(矩阵计算的库)\n",
    "pandas(数据分析的库，一般和numpy结合使用)\n",
    "matplotlib(绘图的库)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据\n",
    "注意，有些数据第一行可能是标题，pandas默认第一行为头，由于本文件没有头\n",
    "所以需要在后面加上header=None，如果想要定义一个头，则在需要添加names=[]的一个数组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先将xy的数据在坐标上画出来，观察其特征（实际上已经知道了是一个线性关系）\n",
    "scatter为散点图的意思，figsize为图像大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先初始化函数，Y=theta1*X+theta0\n",
    "矩阵形式就是theta=[theta0,theeta1]\n",
    "theta就初始化为theta=[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initData(dataName):\n",
    "    data=pandas.read_csv(dataName,header=None).values\n",
    "    data=numpy.array(data)\n",
    "\n",
    "    #data.head()\n",
    "    #data.describe()\n",
    "    #data.plot(kind='scatter', x='x', y='y', figsize=(8,6))\n",
    "    #plt.show()\n",
    "\n",
    "    theta=numpy.ones(data.shape[1])\n",
    "    #theta=[80000,130,-80000]#测试第二个样本数据，把结果直接放在最优得附件，看看下降的过去不，结果发现还是很难到达\n",
    "    print(theta)\n",
    "    return data,theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了theta我们就可以计算当前代价了\n",
    "\n",
    "定义代价函数\n",
    "代价的计算方程为\n",
    "$$J\\left( \\theta  \\right)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}{{{\\left( {{h}_{\\theta }}\\left( {{x}^{(i)}} \\right)-{{y}^{(i)}} \\right)}^{2}}}$$\n",
    "\n",
    "\n",
    "这里的h(x)可以用矩阵成发表示，theta*x==>[0,0]*[1,x]T来表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(data,theta):\n",
    "    cols=data.shape[1]\n",
    "    \n",
    "    x=data[:,0:cols-1]\n",
    "    y=data[:,cols-1]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ycom=numpy.matmul(x,theta)#矩阵内积得到计算出来的ycom\n",
    "\n",
    "    ycom=ycom-y\n",
    "    \n",
    "    cost=numpy.matmul(ycom,ycom)/(2*data.shape[0])#内积，也就是平方求和后除2m\n",
    "\n",
    "    return cost,ycom#保留ycom，后续梯度下降要用到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代价函数计算完后，就可以考虑如何最小化代价了，按照梯度下降的方法，对每个代价函数的每一个theta求导，也就是梯度下降\n",
    "$${{\\theta }_{j}}:={{\\theta }_{j}}-\\alpha \\frac{\\partial }{\\partial {{\\theta }_{j}}}J\\left( \\theta  \\right)$$\n",
    "这里运用求导的方法可以算出导数，然后就可以开始梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的gradientDescent()方法，先计算一次代价，然后利用得到得cost画图看下降过程，ycom来用于梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traitlets import Undefined\n",
    "\n",
    "\n",
    "def gradientDescent(theta,data,alpha,time):\n",
    "    \n",
    "    #插入常数1，便于矩阵计算\n",
    "    #data.insert(0,'ones',1)\n",
    "\n",
    "    ones=numpy.ones(data.shape[0])\n",
    "\n",
    "    data=numpy.c_[ones,data]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #列数\n",
    "    cols=data.shape[1]\n",
    "    #行数,样本数量\n",
    "    row=data.shape[0]\n",
    "\n",
    "    costs=[]\n",
    "    for i in range(time):#下降100次\n",
    "        #计算当前代价\n",
    "        cost,ycom=computeCost(data,theta)\n",
    "        costs.append(cost)\n",
    "        \n",
    "        x=data[:,0:cols-1]\n",
    "        x=numpy.transpose(x)#转置一下\n",
    "\n",
    "        sum=numpy.matmul(x,ycom)/row#sum除以m(就是样本量row),这个就是梯度下降的微分部分\n",
    "        \n",
    "        theta=theta-alpha*numpy.transpose(sum)\n",
    "    return theta,costs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主运行代码如下\n",
    "调用gradientDescent()方法开始梯度下降，返回theta也就是方程参数，costs每次下降后的代价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.]\n",
      "[-3.89578088  1.19303364]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main01():\n",
    "    data,theta=initData('ex1data1.txt')#读取数据\n",
    "    theta,costs=gradientDescent(theta,data,0.02,10000)\n",
    "    print(theta)\n",
    "\n",
    "    # #画costs图观察下降趋势\n",
    "    # plt.figure(figsize=(8, 6), dpi=100)\n",
    "    plt.plot(costs)\n",
    "    plt.show()\n",
    "\n",
    "    # #画样本散点和拟合后的方程，观察拟合情况\n",
    "    # plt.figure(figsize=(8, 6), dpi=100)\n",
    "    # plt.scatter(data.x,data.y)\n",
    "    # x=numpy.arange(0,25,0.1)\n",
    "    # y=theta[1]*x+theta[0]\n",
    "    # plt.plot(x,y)\n",
    "    # plt.show()\n",
    "main01()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以的，有了一元一次方程线性回归的经验，二元一次不是问题\n",
    "## 多变量线性回归\n",
    "在开始多元前，整理一下前面的代码，把复用的整理成方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行第二个二元一次回归出现了指数溢出，猜测原因是样本没有特征缩放\n",
    "果然，当我把下降比列调的很小时（0.0000001），就运行成功了\n",
    "但是和正规方程求得theta还差很多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "特征缩放，第一种和第二种都是\n",
    "第一个是样本减去平均值后除以方差\n",
    "第二个是样本减去平均值后除以平均值\n",
    "虽然不知道效果怎么样，但下降率确实变快了，0.01就可以了\n",
    "'''\n",
    "def featureScaling(data):\n",
    "    return (data-numpy.mean(data, axis=0))/numpy.std(data,axis=0)\n",
    "    #return (data-numpy.mean(data, axis=0))/numpy.mean(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1.]\n",
      "[-1.08265505e-16  8.84765988e-01 -5.31788197e-02]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main02():\n",
    "    data,theta=initData('ex1data2.txt')#读取数据\n",
    "    data=featureScaling(data)\n",
    "\n",
    "    theta,costs=gradientDescent(theta,data,0.111,100000)\n",
    "    print(theta)\n",
    "\n",
    "    plt.plot(costs)\n",
    "    plt.show()\n",
    "main02()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的是关于一元以及多元使用梯度下降来线性回归的\n",
    "在试试正规方程来线性回归\n",
    "## 正规方程的线性回归\n",
    "笔记上说了，正规方程的时间复杂度为O(n3)，所以当特征量很多的时候推荐梯度下降\n",
    "公式如下\n",
    "$$\\theta ={{\\left( {{X}^{T}}X \\right)}^{-1}}{{X}^{T}}y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.77555756e-17,  8.84765988e-01, -5.31788197e-02])"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyparsing import col\n",
    "\n",
    "\n",
    "def normalEquation(dataName):\n",
    "    #读取数据\n",
    "    data=pandas.read_csv(dataName,header=None).values\n",
    "    data=numpy.array(data)\n",
    "\n",
    "\n",
    "    #测试样本2归一化后与梯度下降得结果比较，theta0差距较大，梯度得到的波动较大\n",
    "    data=featureScaling(data)\n",
    "\n",
    "    #插入theta0(插入一列1)\n",
    "    data=numpy.insert(data,0,numpy.ones((data.shape[0])),axis=1)\n",
    "\n",
    "    #提取x,y\n",
    "    column=data.shape[1]\n",
    "    x=data[:,0:column-1]\n",
    "    y=data[:,column-1]\n",
    "\n",
    "    xT=numpy.transpose(x)\n",
    "    theta=numpy.matmul(numpy.matmul(numpy.linalg.inv(numpy.matmul(xT,x)),xT),y)\n",
    "    return theta\n",
    "\n",
    "normalEquation('ex1data2.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1daca1ebfa99d363f49187de6dd878fa6001e85bc2f160c60bcd81d0df0caa1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
